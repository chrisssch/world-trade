{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data to parquet\n",
    "\n",
    "Script for converting the downloaded .dat files to .parquet files.\n",
    "\n",
    "http://localhost:4040/\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and configuration\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Python\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and configure Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "conf = SparkConf().setAppName(\"WorldTrade\").setMaster(\"local[4]\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"2g\")\n",
    "conf.set(\"spark.driver.memory\", \"2g\")\n",
    "conf.set(\"spark.executor.memory\", \"2g\") \n",
    "conf.set(\"spark.executor.pyspark.memory\", \"2g\")\n",
    "\n",
    "# Initialization\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlc = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert .dat files to parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema \n",
    "\n",
    "fields = [\n",
    "    StructField(\"REPORTER\", IntegerType(), True),\n",
    "    StructField(\"REPORTER_ISO\", StringType(), True),\n",
    "    StructField(\"PARTNER\", IntegerType(), True),\n",
    "    StructField(\"PARTNER_ISO\", StringType(), True),\n",
    "    StructField(\"TRADE_TYPE\", StringType(), True),\n",
    "    StructField(\"PRODUCT_NC\", StringType(), True),\n",
    "    StructField(\"PRODUCT_SITC\", StringType(), True),\n",
    "    StructField(\"PRODUCT_CPA2002\", StringType(), True),\n",
    "    StructField(\"PRODUCT_CPA2008\", StringType(), True),\n",
    "    StructField(\"PRODUCT_CPA2_1\", StringType(), True),\n",
    "    StructField(\"PRODUCT_BEC\", StringType(), True),\n",
    "    StructField(\"PRODUCT_SECTION\", StringType(), True),\n",
    "    StructField(\"FLOW\", IntegerType(), True),\n",
    "    StructField(\"STAT_REGIME\", IntegerType(), True),\n",
    "    StructField(\"SUPP_UNIT\", StringType(), True),\n",
    "    StructField(\"PERIOD\", StringType(), True),\n",
    "    StructField(\"VALUE_IN_EUROS\", LongType(), True),\n",
    "    StructField(\"QUANTITY_IN_KG\", LongType(), True),\n",
    "    StructField(\"SUP_QUANTITY\", IntegerType(), True)    \n",
    "]\n",
    "\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '2008',\n",
       " '2009',\n",
       " '2010',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2014',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Folders with monthly .dat files\n",
    "os.listdir(\"data/dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004 : conversion to parquet complete\n",
      "2005 : conversion to parquet complete\n",
      "2006 : conversion to parquet complete\n",
      "2007 : conversion to parquet complete\n",
      "2008 : conversion to parquet complete\n",
      "2009 : conversion to parquet complete\n",
      "2010 : conversion to parquet complete\n",
      "2011 : conversion to parquet complete\n",
      "2012 : conversion to parquet complete\n",
      "2013 : conversion to parquet complete\n",
      "2014 : conversion to parquet complete\n",
      "2015 : conversion to parquet complete\n",
      "2016 : conversion to parquet complete\n",
      "2017 : conversion to parquet complete\n",
      "2018 : conversion to parquet complete\n",
      "2019 : conversion to parquet complete\n"
     ]
    }
   ],
   "source": [
    "# Loop over all folders for each year and convert files\n",
    "for year in os.listdir(\"data/dat\"):\n",
    "    source = \"data/dat/\" + year + \"/*.dat\"\n",
    "    destination = \"data/parquet/full\" + year + \".parquet\"\n",
    "    data = spark.read.csv(source, sep=\",\", header=True, schema=schema)\n",
    "    data.coalesce(4).write.parquet(destination, mode=\"overwrite\")\n",
    "    print(year, \": conversion to parquet complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['full2004.parquet',\n",
       " 'full2005.parquet',\n",
       " 'full2006.parquet',\n",
       " 'full2007.parquet',\n",
       " 'full2008.parquet',\n",
       " 'full2009.parquet',\n",
       " 'full2010.parquet',\n",
       " 'full2011.parquet',\n",
       " 'full2012.parquet',\n",
       " 'full2013.parquet',\n",
       " 'full2014.parquet',\n",
       " 'full2015.parquet',\n",
       " 'full2016.parquet',\n",
       " 'full2017.parquet',\n",
       " 'full2018.parquet',\n",
       " 'full2019.parquet']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of parquet files\n",
    "parquet_files = os.listdir(\"data/parquet\")\n",
    "\n",
    "parquet_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store schemas for all parquet files\n",
    "schemas = []\n",
    "for i in range(len(parquet_files)):\n",
    "    schemas.append(spark.read.parquet(\"data/parquet/\"+parquet_files[i]).schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if schemas for all files are identical\n",
    "schemas_identical = np.zeros([len(parquet_files), len(parquet_files)], dtype=bool)   \n",
    "for i in range(len(parquet_files)):\n",
    "    for j in range(len(parquet_files)):\n",
    "        schemas_identical[i][j] = schemas[i] == schemas[j]\n",
    "\n",
    "# Should return True\n",
    "np.all(schemas_identical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
